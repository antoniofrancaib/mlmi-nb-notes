# Overview

Regression $\rightarrow$ predicting a **continuous output** $y^\star$ given an input vector $x^\star \in \mathbb{R}^D$

Goal: to find a function $f: \mathbb{R}^D \rightarrow \mathbb{R}^{D'}$ to make future predictions $y^\star = f(x^\star)$.

Output space marks the difference between scalar regression or multivariate regression. 
Problems may involve ***interpolation***, where predictions are close to the training data, or ***extrapolation***, where predictions are far from the training data.

### Example Applications

| Application                      | Inputs                                | Outputs                               |
|----------------------------------|---------------------------------------|---------------------------------------|
| Computer-aided drug design       | Representation of molecule            | Biological activity of molecule       |
| Solar power supply forecasting   | Time, physical models, satellite images| Solar power supply                    |
| Informing health care policy     | Health care center characteristics    | Patient treatment outcomes            |
| Image super resolution           | Low resolution image                  | High resolution image                 |
| System identification            | System's state at time $t-1$          | System's state at time $t$            |

# Linear Regression

## Least Squares Fitting

Consider a dataset with $N=10$ scalar input-output pairs $\{(x_{n}, y_{n})\}_{n=1}^N$. We aim to fit a linear model:$$
f(x) = w_1 x + w_0
$$

We estimate $w_0$ and $w_1$ by minimizing the cost function $C_2$, defined as the sum of squared distances between observed outputs and the model predictions:

$$
C_2 = \sum_{n=1}^N [y_n - f(x_n)]^2 = \sum_{n=1}^N [y_n - (w_1 x_n + w_0)]^2 \geq 0
$$

This can be compactly written in matrix notation as:

$$
C_2 = \| \mathbf{y} - \mathbf{X} \mathbf{w} \|^2 = (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w})
$$

where:

$$
\mathbf{y} = \begin{pmatrix}
y_1\\
y_2\\
\vdots \\
y_N
\end{pmatrix}, \quad
\mathbf{X} =  \begin{pmatrix}
1 & x_1\\
1 & x_2\\
\vdots & \vdots \\
1 & x_N
\end{pmatrix}, \quad
\mathbf{w} =  \begin{pmatrix}
w_0\\
w_1
\end{pmatrix}
$$

To minimize $C_2$, we differentiate and set the derivative to zero, resulting in the normal equation:

$$
\frac{\partial C_2}{\partial \mathbf{w}} = -2\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\mathbf{w}) = 0 \implies \mathbf{X}^\top\mathbf{X}\mathbf{w} = \mathbf{X}^\top\mathbf{y}
$$

Solving for $\mathbf{w}$, we get the least squares solution:

$$
\boxed{\mathbf{w} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}}
$$

The Moore-Penrose pseudoinverse generalizes this inverse for non-square matrices.

### Maximum Likelihood Fitting

Assume the data follows a linear model corrupted by Gaussian noise:

$$
y_n = w_0 + w_1 x_n + \epsilon_n, \quad \text{where} \quad \epsilon_n \sim \mathcal{N}(0, \sigma_y^2)
$$

The likelihood of $\mathbf{y}$ given $\mathbf{X}$, $\mathbf{w}$, and $\sigma_y^2$ is:

$$
p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}, \sigma_y^2) = \frac{1}{(2\pi \sigma_y^2)^{N/2}}\exp\left(-\frac{1}{2\sigma_y^2} (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w})\right)
$$

The **log-likelihood** is:

$$
\mathcal{L}(\mathbf{w}) = \log p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}, \sigma_y^2) = -\frac{N}{2} \log(2\pi \sigma_y^2) - \frac{1}{2 \sigma_y^2} (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w})
$$

### Summary

1. The sum-of-squared-errors is a sensible measure of fit due to its closed-form solution and probabilistic interpretation.
2. Deriving the least squares estimate minimizes the squared error assuming Gaussian noise.
3. Least squares estimation is equivalent to maximum likelihood under Gaussian noise assumption.

### Questions

1. **Probabilistic models for regression**
   - Suggest models for two datasets and provide parameter settings.

2. **Maximum-likelihood learning for a simple regression model**
   - Derive the log-likelihood, and compute the maximum likelihood estimate for $a$.

3. **Maximum-likelihood learning for multi-output regression**
   - Derive the log-likelihood, compute the maximum likelihood estimate, and discuss the usefulness of additional outputs.

---
# Non-Linear Regression Using Basis Functions

This section extends linear regression to handle datasets with non-linear relationships by utilizing linear combinations of non-linear basis functions.

## Dataset

Consider a non-linear dataset with inputs and outputs:

```python
x_nonlin = np.load('reg_nonlin_x.npy')
y_nonlin = np.load('reg_nonlin_y.npy')
```

A scatter plot visualizes the dataset:

```python
plt.scatter(x_nonlin, y_nonlin, marker='x', color='red')
beautify_plot({"title": "Non-linear dataset", "x":"$x$", "y":"$y$"})
plt.show()
```

## Modelling Non-Linear Functions

The non-linear function is modeled as a linear combination of basis functions \\(\phi_d(x)\\):

$$
y_n = f(x_n) + \epsilon_n \quad \text{where} \quad \epsilon_n \sim \mathcal{N}(0, \sigma_{y}^2)
$$

$$
f(x_n) = w_0 + w_1 \phi_{1}(x_n) + w_2 \phi_{2}(x_n) + \cdots + w_D \phi_{D}(x_n)
$$

Suitable basis functions include:
- Polynomials: \\(\phi_{d}(x) = x^d\\)
- Sinusoids: \\(\phi_{d}(x) = \cos(\omega_d x + \phi_d)\\)
- Gaussian bumps: \\(\phi_d(x) = \exp(-(x - \mu_d)^2/(2\sigma^2) )\\)

## Least Squares and Maximum Likelihood Fitting

The model remains linear in parameters. Define parameter vector \\(\mathbf{w}\\) and basis function vector \\(\boldsymbol{\phi}(x_n)\\):

$$
\mathbf{w} = [w_0, w_1, \ldots, w_D]^\top
$$

$$
\boldsymbol{\phi}(x_n) = [1, \phi_1(x_n), \ldots, \phi_D(x_n)]^\top
$$

So the model becomes:

$$
y_n = \boldsymbol{\phi}(x_n)^\top \mathbf{w} + \epsilon_n
$$

Collect observations into vectors \\(\mathbf{y}\\) and \\(\boldsymbol{\epsilon}\\):

$$
\mathbf{y} = \boldsymbol{\Phi}\mathbf{w} + \boldsymbol{\epsilon}
$$

Where the design matrix \\(\boldsymbol{\Phi}\\) has entries \\(\phi_d(x_n)\\):

$$
\boldsymbol{\Phi} = \begin{pmatrix}
1 & \phi_1(x_1) & \cdots & \phi_D(x_1)\\
1 & \phi_1(x_2) & \cdots & \phi_D(x_2)\\
\vdots & \vdots & \ddots & \vdots \\
1 & \phi_1(x_N) & \cdots & \phi_D(x_N)
\end{pmatrix}
$$

### Least Squares Error

Minimize the squared error:

$$
C_2 = \big|\big| \mathbf{y} - \boldsymbol{\Phi}\mathbf{w} \big|\big|^2 = (\mathbf{y} - \boldsymbol{\Phi}\mathbf{w})^\top (\mathbf{y} - \boldsymbol{\Phi}\mathbf{w})
$$

### Maximum Likelihood

Negative log-likelihood:

$$
- \mathcal{L}(\mathbf{w}) = \frac{N}{2}\log(2\pi \sigma^2) + \frac{1}{2\sigma^2}(\mathbf{y} - \boldsymbol{\Phi}\mathbf{w})^\top (\mathbf{y} - \boldsymbol{\Phi}\mathbf{w})
$$

Both approaches yield the same solution for weights:

$$
\boxed{\mathbf{w} = \big( \boldsymbol{\Phi}^\top \boldsymbol{\Phi} \big)^{-1} \boldsymbol{\Phi}^\top \mathbf{y}}
$$

### Polynomial Basis Functions (D=5)

```python
D = 5
phi = np.array([[x_ ** d for d in range(D + 1)] for x_ in x_nonlin])
w = np.linalg.solve((phi.T).dot(phi), (phi.T).dot(y_nonlin))
xs = np.linspace(-0.2, 1.2, 100)
phi_pred = np.array([[x_ ** d for d in range(D + 1)] for x_ in xs])
ys = phi_pred.dot(w)
```

### Gaussian Basis Functions (D=5, \(\sigma^2_{\phi} = 0.05\))

```python
D = 5
var_phi = 0.05
phi = np.array([[np.exp(-1/(2*var_phi)*np.power(x_-d/D,2)) for d in range(D + 1)] for x_ in x_nonlin])
w = np.linalg.solve((phi.T).dot(phi), (phi.T).dot(y_nonlin))
xs = np.linspace(-0.2, 1.2, 100)
phi_pred = np.array([[np.exp(-1/(2*var_phi)*np.power(x_-d/D,2)) for d in range(D + 1)] for x_ in xs])
ys = phi_pred.dot(w)
```

## Model Comparison

Both models yielded similar predictions with training costs \\(\mathcal{C}_2 \approx 0.1\\). The extrapolation behavior differed significantly for each basis function type.

## Sensitivity to Hyperparameters

Experiment with:
- Order of the polynomial (\(D\))
- Number of Gaussian basis functions (\(D\))
- Width of basis functions (\(\sigma^2_{\phi}\))

## Summary

Non-linear regression can effectively be handled by transforming inputs using basis functions. The model interpolates based on the chosen basis functions.

## Questions

### 1. Generalization of Models

<details>
<summary>Answer</summary>
The polynomial model explodes based on the highest order term as \(x \rightarrow w_D x^D\). Gaussian basis functions decay to a constant function \(f(x) \rightarrow w_0\) with a decay scale related to basis function width \(\approx \sigma_{\phi}\).
</details>

### 2. Basis Functions in High Dimensions

<details>
<summary>Answer</summary>
Approximately \(\mathcal{O}((L/l)^K)\) basis functions are required, indicating an exponential growth with respect to dimensionality.
</details>

### 3. Adaptive Basis Functions

<details>
<summary>Answer</summary>
Centre \(\mu_d\) could be selected via random subsets or clustering techniques (e.g., k-means), and widths \(\sigma_d^2\) could be chosen based on the median distance from each centre to data points.
</details>

### 4. Sinusoidal Basis Functions

```python
D = 5
omega = np.pi * np.arange(1, D+1)
phi = np.column_stack([np.sin(omega * x_nonlin[:, np.newaxis]), np.cos(omega * x_nonlin[:, np.newaxis])])
w = np.linalg.solve(phi.T.dot(phi), phi.T.dot(y_nonlin))
xs = np.linspace(-0.2, 1.2, 100)
phi_pred = np.column_stack([np.sin(omega * xs[:, np.newaxis]), np.cos(omega * xs[:, np.newaxis])])
ys = phi_pred.dot(w)
print(f'Sum squared errors for D = {D} sinusoidal basis functions: {np.sum((phi.dot(w) - y_nonlin) ** 2):.8f}')
```

## Conclusion

Experimentation with different basis functions and hyperparameters can help optimize non-linear regression models. Advanced models like kernel machines or Gaussian processes might be suitable for complex high-dimensional data.

--- 
# Summary in Markdown with Mathematical Focus

## Bayesian Linear Regression

### Probabilistic Model

The probabilistic model for Bayesian linear regression can be summarized as a series of steps involving Gaussian distributions:

1. Sample weights \(\mathbf{w}^{(m)} \sim \mathcal{N}(\mathbf{0},\sigma_{\mathbf{w}}^2 \mathrm{I})\) for \(m=1,...M\).
2. Define the regression function \(f_{\mathbf{w}}^{(m)}(\mathbf{x})=\boldsymbol{\phi}(\mathbf{x})^\top \mathbf{w}^{(m)}\).
3. Sample \(N\) input locations \(\mathbf{x}^{(m)}_n \sim p(\mathbf{x})\) for \(n=1,...N\).
4. Sample \(N\) output locations \(y_n |\mathbf{w}^{(m)},\mathbf{x}^{(m)}_n,\sigma_{y}^2 \sim \mathcal{N}(f^{(m)}_{\mathbf{w}}(\mathbf{x}^{(m)}_n),\sigma_{y}^2)\) for \(n=1,...N\).

The full probabilistic model is expressed as:

$$
p(\mathbf{w},\mathbf{y},\mathbf{X} | \sigma_{\mathbf{w}}^2,\sigma_{y}^2) = p(\mathbf{w}| \sigma_{\mathbf{w}}^2) p(\mathbf{X}) p(\mathbf{y}|\mathbf{X},\sigma_{y}^2)
  = p(\mathbf{w} | \sigma_{\mathbf{w}}^2) \prod_{n=1}^N p(x_n) p(y_n |\mathbf{w},\mathbf{x}_n,\sigma_{y}^2)
$$

Given the assumptions, the Gaussian prior over the weights and independent observation noise are expressed as:

$$
p(\mathbf{w}| \sigma_{\mathbf{w}}^2) = \mathcal{N}(\mathbf{w}; \mathbf{0},\sigma_{\mathbf{w}}^2 \mathrm{I})
$$

$$
p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \sigma_y^2) = \mathcal{N}(\mathbf{y}; \boldsymbol{\Phi}\mathbf{w}, \sigma_{y}^2 \mathrm{I})
$$

### Probabilistic Inference for the Weights

Using Bayesian inference, the posterior distribution of the weights given observed data is:

$$
p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \sigma_{\mathbf{w}}^2, \sigma_{y}^2) \propto p(\mathbf{w}| \sigma_{\mathbf{w}}^2) p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \sigma_y^2)
$$

Substituting the Gaussian forms for the prior and likelihood:

$$
p(\mathbf{w}| \sigma_{\mathbf{w}}^2) = \frac{1}{(2\pi \sigma_{\mathbf{w}}^2)^{D/2}} \exp\left(-\frac{1}{2\sigma_w^2}\mathbf{w}^\top \mathbf{w}\right)
$$

$$
p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \sigma_y^2) = \frac{1}{(2\pi \sigma_y^2)^{N/2}} \exp\left(-\frac{1}{2\sigma_y^2}(\mathbf{y} - \boldsymbol{\Phi}\mathbf{w})^\top (\mathbf{y} - \boldsymbol{\Phi})\right)
$$

The posterior distribution is also Gaussian:

$$
p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \sigma_{\mathbf{w}}^2, \sigma_{y}^2) = \mathcal{N}(\mathbf{w}; \mathbf{\mu}_{\mathbf{w} | \mathbf{y}, \mathbf{X} },\Sigma_{\mathbf{w} | \mathbf{y}, \mathbf{X} })
$$

where:

$$
\Sigma_{\mathbf{w} | \mathbf{y}, \mathbf{X} }  = \left( \frac{1}{\sigma_y^2} \boldsymbol{\Phi}^\top \boldsymbol{\Phi} + \frac{1}{\sigma_{\mathbf{w}}^2} \mathrm{I} \right)^{-1}
$$

$$
\mathbf{\mu}_{\mathbf{w} | \mathbf{y}, \mathbf{X} } =  \Sigma_{\mathbf{w} | \mathbf{y}, \mathbf{X} } \frac{1}{\sigma_y^2} \boldsymbol{\Phi}^\top \mathbf{y}
$$

### Probabilistic Inference for Prediction

To make predictions for an unseen input \(\mathbf{x}^*\):

$$
p(y^* | \mathbf{x}^*, \mathbf{y},\mathbf{X},\sigma_y^2,\sigma_{\mathbf{w}}^2) = \int p(y^* | \mathbf{x}^*, \mathbf{w},\sigma_y^2) p(\mathbf{w}|\mathbf{y},\mathbf{X},\sigma_y^2,\sigma_{\mathbf{w}}^2) d\mathbf{w}
$$

Given the Gaussian nature of the posterior over \(\mathbf{w}\) and the linear transform \(y^* = \boldsymbol{\phi}_{\ast}^\top \mathbf{w} + \epsilon'\) with \(\epsilon' \sim \mathcal{N}(0,\sigma_y^2)\), the predictive distribution is also Gaussian:

$$
p(y^* | \mathbf{x}^*, \mathbf{y},\mathbf{X},\sigma_y^2,\sigma_{\mathbf{w}}^2)  = \mathcal{N}(y^* ; \mu_{y^*|\mathbf{y},\mathbf{X}},\sigma^2_{y^*| \mathbf{y},\mathbf{X}})
$$

where:

$$
\mu_{y^*|\mathbf{y},\mathbf{X}} = \boldsymbol{\phi}_{\ast}^\top \mu_{\mathbf{w}| \mathbf{y},\mathbf{X}}
$$

$$
\sigma^2_{y^*| \mathbf{y},\mathbf{X}} = \boldsymbol{\phi}_*^\top \Sigma_{\mathbf{w}| \mathbf{y},\mathbf{X}} \boldsymbol{\phi}_* + \sigma_{y}^2
$$

### Summary

This model shows that computing the posterior of the weights and the predictive distribution under Bayesian linear regression can lead to more informative decision-making by accounting for uncertainty in parameter estimates. The unified mathematical formulation allows clear understanding and practical implementation, providing a probabilistic framework that is both robust and interpretable.